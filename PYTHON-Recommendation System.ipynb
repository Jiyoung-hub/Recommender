{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Book Recommendation\n",
    "\n",
    "1. There are two data sets; 1) reviews_Books_5 2) metadata\n",
    "2. The datasets contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014\n",
    "3. It takes about an hour to retrieve each dataset as it contains 8,898,041 reviews (large volume)\n",
    "\n",
    "\"\"\"\n",
    "1) reviews_Books_5 data contains following information: \n",
    "reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "asin - ID of the product, e.g. 0000013714\n",
    "reviewerName - name of the reviewer\n",
    "helpful - helpfulness rating of the review, e.g. 2/3\n",
    "reviewText - text of the review\n",
    "overall - rating of the product\n",
    "summary - summary of the review\n",
    "unixReviewTime - time of the review (unix time)\n",
    "reviewTime - time of the review (raw)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "2) meta data contains following information:\n",
    "asin - ID of the product, e.g. 0000031852\n",
    "title - name of the product\n",
    "price - price in US dollars (at time of crawl)\n",
    "imUrl - url of the product image\n",
    "related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "salesRank - sales rank information\n",
    "brand - brand name\n",
    "categories - list of categories the product belongs to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import correlation, cosine\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import sys, os\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first file: reviews_Books_5\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Books_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NULL values: only the rows having NULL are displayed\n",
    "# Only 'reviewerName' and 'reviewText' contains null value\n",
    "\n",
    "print(df.isnull().sum())\n",
    "(df.isnull().sum() / len(df)).plot(kind='bar', figsize=(10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data check\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average rating in terms of overall score and rating counts\n",
    "\n",
    "average_rating=pd.DataFrame(df.groupby('asin')['overall'].mean())\n",
    "average_rating['ratingCount']= pd.DataFrame(df.groupby('asin')['overall'].count())\n",
    "average_rating.sort_values('ratingCount',ascending=False).head()\n",
    "average_rating.sort_values('ratingCount',ascending=False)[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure statistical significance, users with less than 400 ratings, and books with less than 400 ratings are excluded\n",
    "\n",
    "user_counts=df['reviewerID'].value_counts()\n",
    "df=df[df['reviewerID'].isin(user_counts[user_counts>=400].index)]\n",
    "overall_counts=df['overall'].value_counts()\n",
    "df=df[df['overall'].isin(overall_counts[overall_counts>=400].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified data as df.xlsx\n",
    "\n",
    "df.to_excel(r'C:\\Users\\hahas\\df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the second file: metadata\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('metadata.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data from 'metadata' file\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the modified data 'df.xlsx' as 'ratings'\n",
    "\n",
    "ratings = pd.read_excel (r'C:\\Users\\hahas\\df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two files ('ratings' from the 1st dataset & 'df' from the 2nd dataset)\n",
    "# This is mainly for getting information(e.g. the book title) that the first dataset does not contain\n",
    "\n",
    "book_rating_combined = pd.merge(ratings,df,on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_rating_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NULL values for the combnied data: only the rows having NULL are displayed\n",
    "\n",
    "book_rating_combined.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the size of data, drop the unnecessary columns\n",
    "\n",
    "columns = ['Unnamed: 0','reviewerName','helpful', 'reviewText','summary', 'unixReviewTime', 'reviewTime','imUrl','salesRank','brand']\n",
    "book_rating_modified=book_rating_combined.drop(columns, axis=1)\n",
    "book_rating_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of rows with 'no titles'\n",
    "# This is because if a row does not contain the book title, ratings score would not be meaningful\n",
    "\n",
    "book_rating_modified=book_rating_modified.dropna(axis=0,subset=['title'])  \n",
    "book_rating_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_rating_modified.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_rating_modified.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column that contains'the number of ratings each book received'\n",
    "# This is to ensure that we limit books with at least 'n' ratings (This will be done after a few more steps)\n",
    "\n",
    "book_ratingCount = (book_rating_modified.groupby(by=['title'])['overall'].count().reset_index().rename\n",
    "                    (columns={'overall':\"totalRatingcount\"})[['title','totalRatingcount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_ratingCount.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(book_rating_modified.title.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the newly created column (total_Rating_count) to the exisiting dataset \n",
    "\n",
    "book_rating_modified_total = book_rating_modified.merge(book_ratingCount, left_on = 'title', right_on = 'title', how='left')\n",
    "book_rating_modified_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "book_rating_modified_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the statistics of total_Rating_count\n",
    "# Each book has the average of 3.2 rating_counts (this is not about the score, but reveals the number of ratings received)\n",
    "# The median book has been rated for twice \n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(book_ratingCount['totalRatingcount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below shows the top of the distribution \n",
    "# About 1% of the books received 20 or more ratings\n",
    "\n",
    "print(book_ratingCount['totalRatingcount'].quantile(np.arange(.9,1,.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to books that have received 5 or more ratings\n",
    "\n",
    "popularity_threshold = 5\n",
    "book_rating_filtered=book_rating_modified_total.query('totalRatingcount > @popularity_threshold')\n",
    "book_rating_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With selected data, look into the ratings distribution\n",
    "# The ratings are unevenly distributed and the vast majority of ratings are 4-5\n",
    "# according to the following plot\n",
    "\n",
    "plt.rc(\"font\", size =15)\n",
    "book_rating_filtered.overall.value_counts(sort=True).plot(kind='bar')\n",
    "plt.title('Rating Distribution\\n')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('system1.png',bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (method1) Top 10 Recommendations based on rating counts & scores \n",
    "\n",
    "- a basic recommendation system based on books' popularity by simply counting ratings. \n",
    " - Books with more ratings are considered to be more popular in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Books with the higher score\n",
    "new_ratings_count = pd.DataFrame(book_rating_filtered.groupby(['title'])['overall'].sum())\n",
    "top10 =  new_ratings_count.sort_values('overall',ascending=False).head(10)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Books with the higher counts\n",
    "rating_counts = pd.DataFrame(book_rating_filtered.groupby('title')['overall'].count())\n",
    "top10_c = rating_counts.sort_values('overall', ascending=False).head(10)\n",
    "top10_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (method2) Collaborative-filtering based recommendation \n",
    "\n",
    "## Part 1 - Build and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the Collaborative-filtering model,\n",
    "# Create a Pivot table and Matrix and Fill the missing values with zeros \n",
    "# Using 2D matrix to compute distance between vectors \n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "book_rating_filtered_pivot = book_rating_filtered.pivot_table(index = 'title',columns='reviewerID', values='overall').fillna(0)\n",
    "book_rating_filtered_matrix = csr_matrix(book_rating_filtered_pivot.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rating_filtered_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the k-Nearest Neighbors, we will use the metric 'cosine' and algorithm 'brute'\n",
    "# This will compute the cosine similarity between vectors.\n",
    "\n",
    "model_knn=NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "model_knn.fit(book_rating_filtered_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this model with a random book. \n",
    "# The kNN algorithm will return 5 similar books measuring distances to determine the 'similarities', given one random book\n",
    "\n",
    "query_index = np.random.choice(book_rating_filtered_pivot.shape[0])\n",
    "distances, indices = model_knn.kneighbors(book_rating_filtered_pivot.iloc[query_index,:].values.reshape(1,-1), n_neighbors=6)\n",
    "\n",
    "for i in range(0,len(distances.flatten())):\n",
    "    if i == 0:\n",
    "        print('Book Recommendations for {0}:\\n'.format(book_rating_filtered_pivot.index[query_index]))\n",
    "    else:\n",
    "        print('{0}: {1}, with distance of {2}'.format(i,book_rating_filtered_pivot.index[indices.flatten()[i]],distances.flatten()[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Define the functions based on item-based approach\n",
    "\n",
    "## 1) find k Nearest items 2) predict the reviewer’s rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the generalized version of the previous test model \n",
    "# in terms that now we will take a input value of (a speific book title & an integer k)\n",
    "# rather than randomly testing the model\n",
    "# Likewise, this function will return k books with the similarities value, according to the given item\n",
    "\n",
    "def findkitems(book_input,k):\n",
    "    query_index = book_rating_filtered_pivot.index.get_loc(book_input)\n",
    "    similarities=[]\n",
    "    indices=[]\n",
    "    model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    model_knn.fit(book_rating_filtered_pivot)\n",
    "\n",
    "    distances, indices = model_knn.kneighbors(book_rating_filtered_pivot.iloc[query_index, :].values.reshape(1, -1), n_neighbors = k+1)\n",
    "    similarities = 1-distances.flatten()\n",
    "    print ('{0} most similar items for item {1}:\\n'.format(k,book_rating_filtered_pivot.index[query_index]))\n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if indices.flatten()[i]== query_index:\n",
    "            continue;\n",
    "        else:\n",
    "            print( '{0}: {1} :, with similarity of {2}'.format(i,book_rating_filtered_pivot.index[indices.flatten()[i]], similarities.flatten()[i]))\n",
    "    return similarities,indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities,indices=findkitems(\"The Hunger Games (The Hunger Games, Book 1)\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'predict_itembased' predicts ratings based on item-based CF approach\n",
    "# The rating for target-item 'i' for active reviewer 'a' can be predicted by using a simple weighted average\n",
    "# Therefore, it will return a predicted rating that reviewer 'a' will give item 'i'\n",
    "\n",
    "def predict_itembased(book_input,reviewer):\n",
    "    query_index = book_rating_filtered_pivot.index.get_loc(book_input)\n",
    "    query_index_reviewer = book_rating_filtered_pivot.columns.get_loc(reviewer)\n",
    "    k=5\n",
    "    prediction= wtd_sum =0\n",
    "    similarities, indices=findkitems(book_input,k) #similar users based on correlation coefficients\n",
    "    sum_wt = np.sum(similarities)-1\n",
    "    product=1\n",
    "    \n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if indices.flatten()[i] == query_index:\n",
    "            continue;\n",
    "        else:\n",
    "            product = book_rating_filtered_pivot.iloc[indices.flatten()[i],query_index_reviewer] * (similarities[i])\n",
    "            wtd_sum = wtd_sum + product                              \n",
    "    prediction = int(round(wtd_sum/sum_wt))\n",
    "    print('\\nPredicted rating for reviwer {0} -> {1}: {2}'.format(reviewer,book_rating_filtered_pivot.index[query_index],prediction))      \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction = predict_itembased(\"The Hunger Games (The Hunger Games, Book 1)\",\"A33C08C20U6DJ0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function 'predict_itembased' predicts ratings based on item-based CF approach\n",
    "# The rating for target-item 'i' for active reviewer 'a' can be predicted by using a simple weighted average\n",
    "# Therefore, it will return a predicted rating that reviewer 'a' will give item 'i'\n",
    "# This function excludes ratings with 0 value when computing wtd_sum & sum_wt\n",
    "\n",
    "def predict_itembased_excluding_zero(book_input,reviewer):\n",
    "    query_index = book_rating_filtered_pivot.index.get_loc(book_input)\n",
    "    query_index_reviewer = book_rating_filtered_pivot.columns.get_loc(reviewer)\n",
    "    k=5\n",
    "    prediction= wtd_sum =0\n",
    "    similarities, indices=findkitems(book_input,k) #similar users based on correlation coefficients\n",
    "    product=1\n",
    "    sum_wt = 0\n",
    "    \n",
    "    for i in range(0, len(indices.flatten())):\n",
    "        if indices.flatten()[i] == query_index:\n",
    "            continue;\n",
    "        elif book_rating_filtered_pivot.iloc[indices.flatten()[i],query_index_reviewer] == 0:\n",
    "            continue;\n",
    "        elif book_rating_filtered_pivot.iloc[indices.flatten()[i],query_index_reviewer] !=0:\n",
    "            product = book_rating_filtered_pivot.iloc[indices.flatten()[i],query_index_reviewer] * (similarities[i])\n",
    "            sims = similarities[i]\n",
    "            sum_wt = sum_wt + sims\n",
    "            wtd_sum = wtd_sum + product                              \n",
    "    prediction = int(round(wtd_sum/sum_wt))\n",
    "    print('\\nPredicted rating for reviwer {0} -> {1}: {2}'.format(reviewer,book_rating_filtered_pivot.index[query_index],prediction))      \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict_itembased_excluding_zero(\"The Hunger Games (The Hunger Games, Book 1)\",\"A33C08C20U6DJ0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
